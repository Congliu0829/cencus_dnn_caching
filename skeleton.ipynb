{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# two intermediate layers network\n",
    "class BasicNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        # For your dataset this should be 1\n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers = 4\n",
    "        \n",
    "        self.n_features = {1: num_features, \n",
    "                           2: int(num_features/2), \n",
    "                           3: int(num_features/4), \n",
    "                           4: num_classes}\n",
    "        \n",
    "        self.lin = {i : torch.nn.Linear(self.n_features[i], self.n_features[i+1]) \n",
    "                   for i in range(4)}\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(3):\n",
    "            x = nn.relu(self.lin[i](x))\n",
    "        # Don't use a ReLU at the end -- this need to be a number in [0,1]\n",
    "        return nn.Sigmoid(self.lin[4](x))\n",
    "            \n",
    "    def get_embedding(self, x, k):\n",
    "        assert k < 4\n",
    "        for i in range(k):\n",
    "            x = nn.relu(self.lin[i](x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Basic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2efd7833fd26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnbr_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0025\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_features' is not defined"
     ]
    }
   ],
   "source": [
    "model = BasicNet(num_features, 1).to(device)\n",
    "nbr_epochs = 10\n",
    "lr = 0.0025\n",
    "weight_decay = 0\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(nbr_epochs):\n",
    "    for (x, y) in pre_model.train_loader:\n",
    "        (x, y) = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        ŷ = model(x) \n",
    "\n",
    "        loss = loss_fn(ŷ, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    def __init__(self, n_layers):\n",
    "        self.embeddings = {i: [] for i in range(n_layers)}\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    @property\n",
    "    def get(self, i):\n",
    "        assert i < len(self.embeddings)\n",
    "        return self.embeddings[i]\n",
    "    \n",
    "    def store(self, x, i):\n",
    "        self.embeddings[i].append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store hidden representation in cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nbr_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cc0597776d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we'll use only the hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpre_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nbr_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "cache = Cache(2) # we'll use only the hidden layers\n",
    "\n",
    "for epoch in range(nbr_epochs):\n",
    "    for (x, y) in pre_model.train_loader:\n",
    "        model.step\n",
    "        \n",
    "        (x, y) = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        h2 = model.get_embedding(x, 2) \n",
    "        cache.store(h2, 2)\n",
    "        \n",
    "        h3 = model.get_embedding(x, 3) \n",
    "        cache.store(h3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, indim, outdim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        \n",
    "        self.hidden_e = nn.Sequential(nn.Linear(indim, indim/2),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(int(indim/2), outdim))\n",
    "        \n",
    "        self.hidden_d = nn.Sequential(nn.Linear(outdim, int(indim/2)),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(int(indim/2), indim))\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.hidden_e(x)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.hidden_d(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE2 = AutoEncoder(model.n_features[2], 4) ## Not sure what's a good output dimensionality here (check based on your model)\n",
    "AE3 = AutoEncoder(model.n_features[3], 4)\n",
    "\n",
    "AE2_loss = nn.MSELoss()\n",
    "AE2_opt = optim.Adam(AE2.parameters(), lr=0.001)\n",
    "\n",
    "AE3_loss = nn.MSELoss()\n",
    "AE3_opt = optim.Adam(AE3.parameters(), lr=0.001)\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    for x in cache.embeddings[1]:\n",
    "        #x_encoded = AE2.encode(x) \n",
    "        #x_decoded = AE2.encode(x_encoded)\n",
    "        AE2_opt.zero_grad()\n",
    "        x_decoded = AE2(x)\n",
    "        l = AE2_loss(x_decoded, x)\n",
    "        AE2_loss.backward()\n",
    "        AE2_opt.step() \n",
    "\n",
    "    for x in cache.embeddings[2]:\n",
    "        #x_encoded = AE3.encode(x) \n",
    "        #x_decoded = AE3.encode(x_encoded)\n",
    "        AE3_opt.zero_grad()\n",
    "        x_decoded = AE3(x)\n",
    "        l = AE3_loss(x_decoded, x)\n",
    "        AE3_loss.backward()\n",
    "        AE3_opt.step() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
