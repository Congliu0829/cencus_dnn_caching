{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Census_income.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Congliu0829/cencus_dnn_caching/blob/master/Census_income.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5jfZbcW4VA6e",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "import os\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import preprocessing\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "# cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZFLqreTWHbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPaTj6FmmryS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "import torch\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip install torch==0.4.1\n",
        "!pip install torchvision==0.2.2\n",
        "!pip install Pillow==6.2.2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "from torch.utils.data.sampler import (SubsetRandomSampler,RandomSampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlGu9iTJtaJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85668da0-fff8-40fb-f641-d155e4f60242"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_cuda = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "    \n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbTxg6Vwtc6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67d92886-919d-42e1-ccca-67fc4e3887f0"
      },
      "source": [
        "full_data = pd.read_csv(\n",
        "    \"/content/adult.csv\",\n",
        "    names=[\n",
        "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
        "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
        "        \"Hours per week\", \"Country\", \"Target\"],\n",
        "        sep=r'\\s*,\\s*',\n",
        "        engine='python', skiprows=1,\n",
        "        na_values=\"?\", dtype={0:int, 1:str, 2:int, 3:str, 4:int, 5: str, 6:str , 7:str ,8:str ,9: str, 10:int, 11:int, 12:int, 13:str,14: str})\n",
        "\n",
        "print('Dataset size: ', full_data.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size:  32561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9VeDTALt6hM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "7d2f1d9c-4123-447e-9350-fa3cd78cc722"
      },
      "source": [
        "print(full_data.head())\n",
        "print(full_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Age  Workclass  fnlwgt  ... Hours per week          Country   Target\n",
            "0   90        \"?\"   77053  ...             40  \"United-States\"  \"<=50K\"\n",
            "1   82  \"Private\"  132870  ...             18  \"United-States\"  \"<=50K\"\n",
            "2   66        \"?\"  186061  ...             40  \"United-States\"  \"<=50K\"\n",
            "3   54  \"Private\"  140359  ...             40  \"United-States\"  \"<=50K\"\n",
            "4   41  \"Private\"  264663  ...             40  \"United-States\"  \"<=50K\"\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "(32561, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JdC0nCCt9a3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str_list=[] \n",
        "for data in [full_data]:\n",
        "    for colname, colvalue in data.iteritems(): \n",
        "        if type(colvalue[1]) == str:\n",
        "            str_list.append(colname) \n",
        "num_list = data.columns.difference(str_list) #seperate str with int in input data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5AHEWt8uEK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "83c618e7-d818-4a90-dc5b-cdc7f0553a70"
      },
      "source": [
        "full_size = full_data.shape[0]\n",
        "print('Dataset size Before pruning: ', full_size)\n",
        "for data in [full_data]:\n",
        "    for i in full_data:\n",
        "        data[i].replace('NaN', np.nan, inplace=True)\n",
        "    data.dropna(inplace=True)\n",
        "real_size = full_data.shape[0]\n",
        "print('Dataset size after pruning: ', real_size)\n",
        "print('We eliminated ', (full_size-real_size), ' datapoints')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size Before pruning:  32561\n",
            "Dataset size after pruning:  32561\n",
            "We eliminated  0  datapoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr2T2UuJuIi6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "cf092dc1-7374-4f18-e8d0-bf25002cf681"
      },
      "source": [
        "full_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>Education</th>\n",
              "      <th>Education-Num</th>\n",
              "      <th>Martial Status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Race</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Capital Gain</th>\n",
              "      <th>Capital Loss</th>\n",
              "      <th>Hours per week</th>\n",
              "      <th>Country</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90</td>\n",
              "      <td>\"?\"</td>\n",
              "      <td>77053</td>\n",
              "      <td>\"HS-grad\"</td>\n",
              "      <td>9</td>\n",
              "      <td>\"Widowed\"</td>\n",
              "      <td>\"?\"</td>\n",
              "      <td>\"Not-in-family\"</td>\n",
              "      <td>\"White\"</td>\n",
              "      <td>\"Female\"</td>\n",
              "      <td>0</td>\n",
              "      <td>4356</td>\n",
              "      <td>40</td>\n",
              "      <td>\"United-States\"</td>\n",
              "      <td>\"&lt;=50K\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>82</td>\n",
              "      <td>\"Private\"</td>\n",
              "      <td>132870</td>\n",
              "      <td>\"HS-grad\"</td>\n",
              "      <td>9</td>\n",
              "      <td>\"Widowed\"</td>\n",
              "      <td>\"Exec-managerial\"</td>\n",
              "      <td>\"Not-in-family\"</td>\n",
              "      <td>\"White\"</td>\n",
              "      <td>\"Female\"</td>\n",
              "      <td>0</td>\n",
              "      <td>4356</td>\n",
              "      <td>18</td>\n",
              "      <td>\"United-States\"</td>\n",
              "      <td>\"&lt;=50K\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>66</td>\n",
              "      <td>\"?\"</td>\n",
              "      <td>186061</td>\n",
              "      <td>\"Some-college\"</td>\n",
              "      <td>10</td>\n",
              "      <td>\"Widowed\"</td>\n",
              "      <td>\"?\"</td>\n",
              "      <td>\"Unmarried\"</td>\n",
              "      <td>\"Black\"</td>\n",
              "      <td>\"Female\"</td>\n",
              "      <td>0</td>\n",
              "      <td>4356</td>\n",
              "      <td>40</td>\n",
              "      <td>\"United-States\"</td>\n",
              "      <td>\"&lt;=50K\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>\"Private\"</td>\n",
              "      <td>140359</td>\n",
              "      <td>\"7th-8th\"</td>\n",
              "      <td>4</td>\n",
              "      <td>\"Divorced\"</td>\n",
              "      <td>\"Machine-op-inspct\"</td>\n",
              "      <td>\"Unmarried\"</td>\n",
              "      <td>\"White\"</td>\n",
              "      <td>\"Female\"</td>\n",
              "      <td>0</td>\n",
              "      <td>3900</td>\n",
              "      <td>40</td>\n",
              "      <td>\"United-States\"</td>\n",
              "      <td>\"&lt;=50K\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>41</td>\n",
              "      <td>\"Private\"</td>\n",
              "      <td>264663</td>\n",
              "      <td>\"Some-college\"</td>\n",
              "      <td>10</td>\n",
              "      <td>\"Separated\"</td>\n",
              "      <td>\"Prof-specialty\"</td>\n",
              "      <td>\"Own-child\"</td>\n",
              "      <td>\"White\"</td>\n",
              "      <td>\"Female\"</td>\n",
              "      <td>0</td>\n",
              "      <td>3900</td>\n",
              "      <td>40</td>\n",
              "      <td>\"United-States\"</td>\n",
              "      <td>\"&lt;=50K\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Workclass  fnlwgt  ... Hours per week          Country   Target\n",
              "0   90        \"?\"   77053  ...             40  \"United-States\"  \"<=50K\"\n",
              "1   82  \"Private\"  132870  ...             18  \"United-States\"  \"<=50K\"\n",
              "2   66        \"?\"  186061  ...             40  \"United-States\"  \"<=50K\"\n",
              "3   54  \"Private\"  140359  ...             40  \"United-States\"  \"<=50K\"\n",
              "4   41  \"Private\"  264663  ...             40  \"United-States\"  \"<=50K\"\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8qXzXWvuKr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "563a3206-0bc5-4a33-af30-09d68eb1646c"
      },
      "source": [
        "full_labels = full_data['Target'].copy()\n",
        "print(full_labels.shape[0])\n",
        "\n",
        "full_data = full_data.drop(['Target'], axis=1)\n",
        "print(full_data.shape[0])\n",
        "\n",
        "# Label Encode Labels\n",
        "label_encoder = LabelEncoder()\n",
        "full_labels = label_encoder.fit_transform(full_labels)\n",
        "print(full_labels[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32561\n",
            "32561\n",
            "[0 0 0 0 0 0 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seJKmQWluOqY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1c0f3c3d-1ea1-4fc7-d984-1435662072c5"
      },
      "source": [
        "cat_data = full_data.select_dtypes(include=['object']).copy()\n",
        "other_data = full_data.select_dtypes(include=['int']).copy()\n",
        "print(cat_data.head())\n",
        "print(cat_data.shape)\n",
        "print(other_data.shape)#seperate data by objects and numbers "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Workclass       Education Martial Status  ...     Race       Sex          Country\n",
            "0        \"?\"       \"HS-grad\"      \"Widowed\"  ...  \"White\"  \"Female\"  \"United-States\"\n",
            "1  \"Private\"       \"HS-grad\"      \"Widowed\"  ...  \"White\"  \"Female\"  \"United-States\"\n",
            "2        \"?\"  \"Some-college\"      \"Widowed\"  ...  \"Black\"  \"Female\"  \"United-States\"\n",
            "3  \"Private\"       \"7th-8th\"     \"Divorced\"  ...  \"White\"  \"Female\"  \"United-States\"\n",
            "4  \"Private\"  \"Some-college\"    \"Separated\"  ...  \"White\"  \"Female\"  \"United-States\"\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "(32561, 8)\n",
            "(32561, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj8LKiL1uRAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newcat_data = pd.get_dummies(cat_data, columns=[\n",
        "    \"Workclass\", \"Education\", \"Country\" ,\"Relationship\", \"Martial Status\", \"Occupation\", \"Relationship\",\n",
        "    \"Race\", \"Sex\"\n",
        "])# apply a encoder for str catagories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij2whrbvuUZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "b5c40a5f-fb37-4d5c-cbcb-017ebf7877ba"
      },
      "source": [
        "newcat_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Workclass_\"?\"</th>\n",
              "      <th>Workclass_\"Federal-gov\"</th>\n",
              "      <th>Workclass_\"Local-gov\"</th>\n",
              "      <th>Workclass_\"Never-worked\"</th>\n",
              "      <th>Workclass_\"Private\"</th>\n",
              "      <th>Workclass_\"Self-emp-inc\"</th>\n",
              "      <th>Workclass_\"Self-emp-not-inc\"</th>\n",
              "      <th>Workclass_\"State-gov\"</th>\n",
              "      <th>Workclass_\"Without-pay\"</th>\n",
              "      <th>Education_\"10th\"</th>\n",
              "      <th>Education_\"11th\"</th>\n",
              "      <th>Education_\"12th\"</th>\n",
              "      <th>Education_\"1st-4th\"</th>\n",
              "      <th>Education_\"5th-6th\"</th>\n",
              "      <th>Education_\"7th-8th\"</th>\n",
              "      <th>Education_\"9th\"</th>\n",
              "      <th>Education_\"Assoc-acdm\"</th>\n",
              "      <th>Education_\"Assoc-voc\"</th>\n",
              "      <th>Education_\"Bachelors\"</th>\n",
              "      <th>Education_\"Doctorate\"</th>\n",
              "      <th>Education_\"HS-grad\"</th>\n",
              "      <th>Education_\"Masters\"</th>\n",
              "      <th>Education_\"Preschool\"</th>\n",
              "      <th>Education_\"Prof-school\"</th>\n",
              "      <th>Education_\"Some-college\"</th>\n",
              "      <th>Country_\"?\"</th>\n",
              "      <th>Country_\"Cambodia\"</th>\n",
              "      <th>Country_\"Canada\"</th>\n",
              "      <th>Country_\"China\"</th>\n",
              "      <th>Country_\"Columbia\"</th>\n",
              "      <th>Country_\"Cuba\"</th>\n",
              "      <th>Country_\"Dominican-Republic\"</th>\n",
              "      <th>Country_\"Ecuador\"</th>\n",
              "      <th>Country_\"El-Salvador\"</th>\n",
              "      <th>Country_\"England\"</th>\n",
              "      <th>Country_\"France\"</th>\n",
              "      <th>Country_\"Germany\"</th>\n",
              "      <th>Country_\"Greece\"</th>\n",
              "      <th>Country_\"Guatemala\"</th>\n",
              "      <th>Country_\"Haiti\"</th>\n",
              "      <th>...</th>\n",
              "      <th>Relationship_\"Not-in-family\"</th>\n",
              "      <th>Relationship_\"Other-relative\"</th>\n",
              "      <th>Relationship_\"Own-child\"</th>\n",
              "      <th>Relationship_\"Unmarried\"</th>\n",
              "      <th>Relationship_\"Wife\"</th>\n",
              "      <th>Martial Status_\"Divorced\"</th>\n",
              "      <th>Martial Status_\"Married-AF-spouse\"</th>\n",
              "      <th>Martial Status_\"Married-civ-spouse\"</th>\n",
              "      <th>Martial Status_\"Married-spouse-absent\"</th>\n",
              "      <th>Martial Status_\"Never-married\"</th>\n",
              "      <th>Martial Status_\"Separated\"</th>\n",
              "      <th>Martial Status_\"Widowed\"</th>\n",
              "      <th>Occupation_\"?\"</th>\n",
              "      <th>Occupation_\"Adm-clerical\"</th>\n",
              "      <th>Occupation_\"Armed-Forces\"</th>\n",
              "      <th>Occupation_\"Craft-repair\"</th>\n",
              "      <th>Occupation_\"Exec-managerial\"</th>\n",
              "      <th>Occupation_\"Farming-fishing\"</th>\n",
              "      <th>Occupation_\"Handlers-cleaners\"</th>\n",
              "      <th>Occupation_\"Machine-op-inspct\"</th>\n",
              "      <th>Occupation_\"Other-service\"</th>\n",
              "      <th>Occupation_\"Priv-house-serv\"</th>\n",
              "      <th>Occupation_\"Prof-specialty\"</th>\n",
              "      <th>Occupation_\"Protective-serv\"</th>\n",
              "      <th>Occupation_\"Sales\"</th>\n",
              "      <th>Occupation_\"Tech-support\"</th>\n",
              "      <th>Occupation_\"Transport-moving\"</th>\n",
              "      <th>Relationship_\"Husband\"</th>\n",
              "      <th>Relationship_\"Not-in-family\"</th>\n",
              "      <th>Relationship_\"Other-relative\"</th>\n",
              "      <th>Relationship_\"Own-child\"</th>\n",
              "      <th>Relationship_\"Unmarried\"</th>\n",
              "      <th>Relationship_\"Wife\"</th>\n",
              "      <th>Race_\"Amer-Indian-Eskimo\"</th>\n",
              "      <th>Race_\"Asian-Pac-Islander\"</th>\n",
              "      <th>Race_\"Black\"</th>\n",
              "      <th>Race_\"Other\"</th>\n",
              "      <th>Race_\"White\"</th>\n",
              "      <th>Sex_\"Female\"</th>\n",
              "      <th>Sex_\"Male\"</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 108 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Workclass_\"?\"  Workclass_\"Federal-gov\"  ...  Sex_\"Female\"  Sex_\"Male\"\n",
              "0              1                        0  ...             1           0\n",
              "1              0                        0  ...             1           0\n",
              "2              1                        0  ...             1           0\n",
              "3              0                        0  ...             1           0\n",
              "4              0                        0  ...             1           0\n",
              "\n",
              "[5 rows x 108 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vKvXxPtuWy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_data = pd.concat([other_data, newcat_data], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMYWsm6_uY1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "cbecab4f-0295-4f11-d0ec-49cfbf75f2df"
      },
      "source": [
        "print(full_data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Age  fnlwgt  Education-Num  ...  Race_\"White\"  Sex_\"Female\"  Sex_\"Male\"\n",
            "0   90   77053              9  ...             1             1           0\n",
            "1   82  132870              9  ...             1             1           0\n",
            "2   66  186061             10  ...             0             1           0\n",
            "3   54  140359              4  ...             1             1           0\n",
            "4   41  264663             10  ...             1             1           0\n",
            "\n",
            "[5 rows x 114 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWZRJLw5ub5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = 20000\n",
        "valid_size = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InW9lsGLuefR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ff9e2ca8-12d5-4ee7-b424-dd6e1f734a7c"
      },
      "source": [
        "train_x = full_data.iloc[:train_size, :]\n",
        "train_y = full_labels[:train_size]\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print()\n",
        "\n",
        "valid_x = full_data.iloc[train_size:(train_size+valid_size), :]\n",
        "valid_y = full_labels[train_size:(train_size+valid_size)]\n",
        "print(valid_x.shape)\n",
        "print(valid_y.shape)\n",
        "print()\n",
        "\n",
        "test_x = full_data.iloc[(train_size+valid_size):, :]\n",
        "test_y = full_labels[(train_size+valid_size):]\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)\n",
        "num_features = test_x.shape[1]\n",
        "print(type(train_x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 114)\n",
            "(20000,)\n",
            "\n",
            "(10000, 114)\n",
            "(10000,)\n",
            "\n",
            "(2561, 114)\n",
            "(2561,)\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_8_RqNVuho_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84f66f95-f10f-4185-e247-0d8898e1cd90"
      },
      "source": [
        "print(train_y[:25])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP8uZn5IujjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c41f8f1f-15c8-4dd2-f353-842251857f8f"
      },
      "source": [
        "num_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLE7X0-oulZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory = os.path.dirname('./data_ML/')\n",
        "\n",
        "try:\n",
        "    os.stat(directory)\n",
        "except:\n",
        "    os.mkdir(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYuTnISYunlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, shutil\n",
        "folder = './data_ML/'\n",
        "for the_file in os.listdir(folder):\n",
        "    file_path = os.path.join(folder, the_file)\n",
        "    try:\n",
        "        if os.path.isfile(file_path):\n",
        "            os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "!ls \"./data_ML/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY2_lyYxuqLz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9a081a3b-b385-4bef-ecd7-15b5cca2d1cc"
      },
      "source": [
        "full_dataset = np.asarray(full_data).astype(np.float32)\n",
        "z=0\n",
        "\n",
        "# load data for dataloader\n",
        "start = time.time()\n",
        "for x in range(full_dataset.shape[0]):\n",
        "    for y in range(2):\n",
        "        if full_labels[x] == y: \n",
        "            temp = (full_dataset[x,:])\n",
        "            \n",
        "            directory = './data_ML/' + str(label_encoder.classes_[y])\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            \n",
        "            \n",
        "            np.save((directory+ '/' +str(z) +'.npy'), temp)\n",
        "            \n",
        "            z += 1\n",
        "    \n",
        "end = time.time()\n",
        "\n",
        "print('Time to process: ', end-start)\n",
        "print(z, ' datapoints saved to path')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to process:  7.95237922668457\n",
            "32561  datapoints saved to path\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsINR60quzDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = np.zeros(num_features)\n",
        "std_mean = np.ones(num_features)\n",
        "\n",
        "\n",
        "class prep_stuff:\n",
        "    def __init__(self, batch_size, path):\n",
        "        self.batch_size = batch_size\n",
        "        self.path = path\n",
        "\n",
        "        batch_size_eval = 128\n",
        "          \n",
        "        self.train_data = datasets.DatasetFolder(self.path, np.load, ('npy') \n",
        "                               )\n",
        "   \n",
        "        \n",
        "        indices = list(range(len(self.train_data)))\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        # Split dataset into train and Test sets\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=self.batch_size,\n",
        "            sampler=SubsetRandomSampler(indices[:20000]),\n",
        "            num_workers=1,\n",
        "        )\n",
        "\n",
        "        self.valid_loader = DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=self.batch_size,\n",
        "            sampler=SubsetRandomSampler(indices[20000:30000]),\n",
        "            num_workers=1,\n",
        "        )\n",
        "\n",
        "        self.test_loader = DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=batch_size_eval,\n",
        "            sampler=SubsetRandomSampler(indices[30000:]),\n",
        "            num_workers=1,\n",
        "        )\n",
        "        \n",
        "           \n",
        "\n",
        "\n",
        "# Create the object with both loader and loss functions\n",
        "\n",
        "batchSize = 1000\n",
        "path = './data_ML/'\n",
        "\n",
        "# 20000 for train\n",
        "# 10000 for valid \n",
        "# 2561 for Test\n",
        "pre_model = prep_stuff(batchSize, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJdp-GhVsKkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# two intermediate layers network\n",
        "class BasicNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.num_classes = num_classes\n",
        "        self.layers = 0\n",
        "        \n",
        "        self.lin1 = torch.nn.Linear(self.num_features,  80)        \n",
        "        self.lin2 = torch.nn.Linear(80, 40)        \n",
        "        self.lin3 = torch.nn.Linear(40, 10) \n",
        "        self.lin4 = torch.nn.Linear(10, self.num_classes)\n",
        "        self.prelu = nn.PReLU()\n",
        "        # self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, xin):\n",
        "        x1 = F.relu(self.lin1(xin)) #input layer\n",
        "        x2 = F.relu(self.lin2(x1))     #intermediate layer1\n",
        "        x3 = F.relu(self.lin3(x2))     #intermediate layer2\n",
        "        x4 = F.relu(self.lin4(x3)) #output layer\n",
        "        # x4 = self.dropout(x4)\n",
        "        return x4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_RHyzsQrb28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_size = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      \n",
        "        for inputs, target in test_loader:\n",
        "            \n",
        "            inputs, target = inputs.to(device), target.to(device)\n",
        "            \n",
        "            output = model(inputs)\n",
        "            test_size += len(inputs)\n",
        "            test_loss += test_loss_fn(output, target).item() \n",
        "            pred = output.max(1, keepdim=True)[1] \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= test_size\n",
        "    accuracy = correct / test_size\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, test_size, 100. * accuracy))\n",
        "    \n",
        "    return test_loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEpBu3dXvaxk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3f33c481-b781-412c-8a80-53d54e727bfc"
      },
      "source": [
        "model = BasicNet(num_features, 2).to(device)\n",
        "nbr_epochs = 10\n",
        "lr = 0.0025\n",
        "weight_decay = 0\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "test_size = 0\n",
        "cache_label = []\n",
        "\n",
        "# Surrogate loss used for training\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr ,weight_decay=weight_decay)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "print('Training beginning...')\n",
        "start_time = time.time()\n",
        "for epoch in range(nbr_epochs):\n",
        "    print('Epoch ', epoch, ':')\n",
        "    for inputs, target in pre_model.train_loader:\n",
        "      inputs, target = inputs.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(inputs) \n",
        "\n",
        "      ############################# cache_label_storage\n",
        "      cache_label.append(target)\n",
        "\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward(retain_graph=True)\n",
        "      optimizer.step()\n",
        "      # loss, acc = test(model, pre_model.valid_loader)\n",
        "    \n",
        "end_time = time.time()\n",
        "print('Training on ' + str(nbr_epochs) + ' epochs done in ', str(end_time-start_time),' seconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training beginning...\n",
            "Epoch  0 :\n",
            "Epoch  1 :\n",
            "Epoch  2 :\n",
            "Epoch  3 :\n",
            "Epoch  4 :\n",
            "Epoch  5 :\n",
            "Epoch  6 :\n",
            "Epoch  7 :\n",
            "Epoch  8 :\n",
            "Epoch  9 :\n",
            "Training on 10 epochs done in  52.30160570144653  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wOrZqkqgmIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#take out the last set of label(target)\n",
        "def last_set(label,all_size,batchsize):\n",
        "  label = label[(len(label)-int(all_size/batchsize)):len(label)]\n",
        "  return label\n",
        "\n",
        "cache_label = last_set(cache_label,20000,1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zmPcoaxbs1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Cache:\n",
        "  def __init__(self,lin):\n",
        "    self.lin = lin\n",
        "  def store(self,cache):\n",
        "    cache.append(self.lin)\n",
        "    return cache\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiq8OsJoYyvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#store the output from hidden layers of basicnet and their corresponding labels\n",
        "#they are basiclin1, basiclin2\n",
        "output1_list = []\n",
        "output2_list = []\n",
        "for inputs, target in pre_model.train_loader:\n",
        "  ori_lin1 = model.lin2(model.lin1(inputs))\n",
        "  # inter_output1 = Cache((ori_lin1,target))#with target\n",
        "  inter_output1 = Cache(ori_lin1)#without target\n",
        "  basiclin1 = inter_output1.store(output1_list)\n",
        "  ori_lin2 = model.lin3(ori_lin1)\n",
        "  # inter_output2 = Cache((ori_lin2,target))#with target\n",
        "  inter_output2 = Cache(ori_lin2)#without target\n",
        "  basiclin2 = inter_output2.store(output2_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19wFElgTx2yS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split(cache):\n",
        "  store_list = []\n",
        "  for i in range(len(cache)):\n",
        "    for each_tensor in cache[i]:\n",
        "      store_list.append(each_tensor)\n",
        "  return store_list\n",
        "\n",
        "basiclin1 = split(basiclin1)\n",
        "basiclin2 = split(basiclin2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lk_abT4443m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalize data in these caches for knn algorithm\n",
        "def normalize(data):\n",
        "  nor_data = []\n",
        "  for i in range(len(data)):\n",
        "    m = np.mean(data[i])\n",
        "    mx = max(data[i])\n",
        "    mn = min(data[i])\n",
        "    nor_data.append((data[i]-m)/(mx-mn))\n",
        "  return nor_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxgsWZLCgos9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct encoder&decoder for each cache (2 in total)\n",
        "class Encoderdecoder(nn.Module):\n",
        "    def __init__(self, in_dim,lin1_dim): \n",
        "        super(Encoderdecoder,self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(in_dim,lin1_dim) \n",
        "        self.linear2 = torch.nn.Linear(lin1_dim,4)\n",
        "        self.linear3 = torch.nn.Linear(4,lin1_dim)\n",
        "        self.linear4 = torch.nn.Linear(lin1_dim,in_dim)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x1 = F.relu(x)\n",
        "        x = self.linear3(x1)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "encoderdecoder1 = Encoderdecoder(40,25).to(device)\n",
        "encoderdecoder2 = Encoderdecoder(10,5).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2gNH-aQ42ci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6de2d556-2a54-43ff-c207-743c4d1e5c2d"
      },
      "source": [
        "# training to reduce dimsension of first cache\n",
        "epoch_dim = 10\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(encoderdecoder1.parameters(),lr=0.1)\n",
        "train_loss1 = []\n",
        "lr = 0.0025\n",
        "weight_decay = 0\n",
        "print('Training of autoencoder1 beginning...')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epoch_dim):\n",
        "  for inputs in basiclin1: #the result from the first layer in basic network\n",
        "    inputs = inputs.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output1 = encoderdecoder1(inputs)                  \n",
        "    loss = criterion(output1, inputs)\n",
        "    train_loss1.append(loss)\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step() \n",
        "\n",
        "end_time = time.time()\n",
        "print('Training on ' + str(nbr_epochs) + ' epochs done in ', str(end_time-start_time),' seconds')\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training beginning...\n",
            "Training on 10 epochs done in  306.94061517715454  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edpfIIHC8s_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#convert tensors to numpy individually\n",
        "def ten2num(cache):\n",
        "  store_list = []\n",
        "  for i in range(len(cache)):\n",
        "    store_list.append(cache[i].detach().numpy())\n",
        "  return store_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhh0by099Egn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss1 = last_set(train_loss1,20000,1)\n",
        "train_loss1_np = ten2num(train_loss1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdBT13BG6B65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "54607295-7ada-473a-866d-0f4da601e7e2"
      },
      "source": [
        "#plot train_loss1 vs i in basiclin1\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "n = 20000\n",
        "X = np.arange(n)+1 #Xæ˜¯1,2,3,4,5,6,7,8,æŸ±çš„ä¸ªæ•°\n",
        "Y1 = train_loss1_np\n",
        "# plt.bar(X, Y1, alpha=0.9, width = 0.35, facecolor = 'lightskyblue', edgecolor = 'white', label='one', lw=1)\n",
        "plt.bar(X, Y1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 20000 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpnW4k5uChax",
        "colab_type": "text"
      },
      "source": [
        "loss in autoencoder 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ7fXvJL-JsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7ffa9d25-8545-4c41-a270-6ba5d4202366"
      },
      "source": [
        "# training to reduce dimsension of first cache\n",
        "epoch_dim = 10\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(encoderdecoder1.parameters(),lr=0.1)\n",
        "train_loss2 = []\n",
        "lr = 0.0025\n",
        "weight_decay = 0\n",
        "print('Training on autoencoder2 beginning...')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epoch_dim):\n",
        "  for inputs in basiclin2: #the result from the first layer in basic network\n",
        "    inputs = inputs.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output2 = encoderdecoder2(inputs)                  \n",
        "    loss = criterion(output2, inputs)\n",
        "    train_loss2.append(loss)\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step() \n",
        "\n",
        "end_time = time.time()\n",
        "print('Training on ' + str(nbr_epochs) + ' epochs done in ', str(end_time-start_time),' seconds')\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training beginning...\n",
            "Training on 10 epochs done in  288.34115195274353  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOBvnvqV_YaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss2 = last_set(train_loss2,20000,1)\n",
        "train_loss2_np = ten2num(train_loss2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JinBEDlX_ok_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "ff64d0ab-6e0d-4f0e-e956-5e8ffbfddc4a"
      },
      "source": [
        "#plot train_loss2 vs i in basiclin2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "n = 20000\n",
        "X = np.arange(n)+1 #Xæ˜¯1,2,3,4,5,6,7,8,æŸ±çš„ä¸ªæ•°\n",
        "Y2 = train_loss2_np\n",
        "# plt.bar(X, Y1, alpha=0.9, width = 0.35, facecolor = 'lightskyblue', edgecolor = 'white', label='one', lw=1)\n",
        "plt.bar(X, Y2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 20000 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW25O3wqCn5P",
        "colab_type": "text"
      },
      "source": [
        "loss in autoencoder2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5tsMt8Ckf0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cache1 = [] #empty list used to store the reduced dimension output from layer 1\n",
        "cache2 = [] #empty list used to store the reduced dimension output from layer 2\n",
        "#store the output from autoencoder \n",
        "#they are cache1_redu,cache2_redu\n",
        "\n",
        "for data1 in basiclin1:\n",
        "  redu_lin1 = encoderdecoder1.linear2(encoderdecoder1.linear1(data1))\n",
        "  redu_output1 = Cache(redu_lin1)\n",
        "  cache1 = redu_output1.store(cache1)\n",
        "\n",
        "for data2 in basiclin2:\n",
        "  redu_lin2 = encoderdecoder2.linear2(encoderdecoder2.linear1(data2))\n",
        "  redu_output2 = Cache(redu_lin2)\n",
        "  cache2 = redu_output2.store(cache2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeMMTFM83AAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert data in cache1,2 from tensor to numpy \n",
        "cache1 = ten2num(cache1)\n",
        "cache2 = ten2num(cache2)\n",
        "cache_label = ten2num(split(last_set(cache_label)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRNrba8ZBOz4",
        "colab_type": "text"
      },
      "source": [
        "code above is  offline phase, the code below is incomplete and is used for freezing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibcl0WAXh51f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testmodel = BasicNet(num_features, 2).to(device)\n",
        "test_accuracy = []\n",
        "train_loss = []\n",
        "nbr_epochs = 10\n",
        "lr = 0.0025\n",
        "weight_decay = 0\n",
        "test_cache_label = [] #empty list used to store label of each test data\n",
        "test_interlin1 = [] #empty list used to store intermediate test output from first layer\n",
        "test_interlin2 = [] #empty list used to store intermediate test output from second layer\n",
        "\n",
        "# Surrogate loss used for training\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr ,weight_decay=weight_decay)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "print('Testing beginning...')\n",
        "start_time = time.time()\n",
        "for epoch in range(1, nbr_epochs+1):\n",
        "    print('Epoch ', epoch, ':')\n",
        "    for inputs, target in pre_model.valid_loader:\n",
        "      inputs, target = inputs.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = testmodel(inputs)  \n",
        "      test_cache_label.append(target)\n",
        "      loss = loss_fn(output[3], target)\n",
        "      loss.backward(retain_graph=True)\n",
        "      optimizer.step()\n",
        "      ######### store output\n",
        "      test_inter_output = Inter_output(output[1],output[2])\n",
        "      testlin1,testlin2 = test_inter_output.store(test_interlin1,test_interlin2)\n",
        "\n",
        "\n",
        "    \n",
        "end_time = time.time()\n",
        "print('Testing on ' + str(nbr_epochs) + ' epochs done in ', str(end_time-start_time),' seconds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoTD3Ax1kDwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#take out the last set of  intermediate output\n",
        "test_last_lin1 = last_set(testlin1,20000,1000)\n",
        "test_last_lin2 = last_set(testlin2,20000,1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu4iaoQGBlEx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d94388a-0f19-4cae-85e4-2f506d2a5045"
      },
      "source": [
        "# training to reduce dimsension of first cache\n",
        "epoch_dim = 10\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(encoderdecoder1.parameters(),lr=0.01)\n",
        "cache3 = [] #empty list used to store the reduced dimension test output from layer 1\n",
        "cache4 = [] #empty list used to store the reduced dimension test output from layer 2\n",
        "\n",
        "for epoch in range(epoch_dim):\n",
        "  running_loss = 0.0\n",
        "  for i in test_last_lin1: #the result from the first layer in basic network\n",
        "    inputs = i.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output3 = encoderdecoder1(inputs)                       \n",
        "    loss = criterion(output3[0], inputs)\n",
        "    loss.backward(retain_graph=True)\n",
        "    running_loss += loss.item() \n",
        "\n",
        "    ##store the reduced dimension output from first layer of basic network\n",
        "    cache_dim3 = Cache(output3[1])\n",
        "    cache_dim3.store(cache3)\n",
        "      \n",
        "\n",
        "\n",
        "    \n",
        "print('Finish Training encoderdecoder1 process')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish Training encoderdecoder1 process\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB1C8gYdBqop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ae00b9a-01c8-4e25-e65b-867eb903266c"
      },
      "source": [
        "# training to reduce dimsension of first cache\n",
        "epoch_dim = 10\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(encoderdecoder2.parameters(),lr=0.01)\n",
        "\n",
        "for epoch in range(epoch_dim):\n",
        "  running_loss = 0.0\n",
        "  for i in test_last_lin2:\n",
        "    inputs = i.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output4 = encoderdecoder2(inputs)\n",
        "    loss = criterion(output4[0], inputs)\n",
        "    loss.backward(retain_graph=True)\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    ##store the reduced dimension output from second layer of basic network\n",
        "    cache_dim4 = Cache(output4[1])\n",
        "    cache_dim4.store(cache4)\n",
        "\n",
        "    \n",
        "print('Finish Training encoderdecoder2 process')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish Training encoderdecoder2 process\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYClQ8PBIPra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#take last set of this test output\n",
        "cache3 = last_set(cache3,10000,1000)\n",
        "cache4 = last_set(cache4,10000,1000)\n",
        "#convert the reduced dimension test output from layer1&layer2 to numpy individually\n",
        "cache3 = ten2num(cache3)\n",
        "cache4 = ten2num(cache4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG54mwDSvuQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cache1 = normalize(cache1)\n",
        "cache2 = normalize(cache2)\n",
        "cache3 = normalize(cache3)\n",
        "cache4 = normalize(cache4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDPq3TVhpPkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "neigh_layer1 = NearestNeighbors(n_neighbors=5) # 5 nearest neighbor\n",
        "neigh_layer1.fit(cache1) #train first layer cache\n",
        "neigh_layer1_list = [] # an empty list used to store the imformation of neighbors of data in cache3\n",
        "for i in range(len(cache3)):\n",
        "  neigh_layer1_list.append(neigh_layer1.kneighbors([cache3[i]]))\n",
        "# length of neigh_layer1_list is 10000ï¼Œeach has 2 elementsï¼Œfirst is the distance of data in cache3ï¼Œsecond is their indices in cache1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plWf_KrBGXua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "times0 = 0\n",
        "times1 = 0\n",
        "list_confidence0 = []\n",
        "list_confidence1 = []\n",
        "distance0 = 0\n",
        "distance1 = 0\n",
        "#initialize\n",
        "for i in range(len(neigh_layer1_list)):\n",
        "  for j in range(5):\n",
        "    if cache_label[neigh_layer1_list[i][1][0][j]] == 1:\n",
        "       times1 = times1 + 1\n",
        "       distance1 = distance1 + 1/neigh_layer1_list[i][0][0][j]\n",
        "    if cache_label[neigh_layer1_list[i][1][0][j]] == 0:\n",
        "       times0 = times0 + 1\n",
        "       distance0 = distance0 + 1/neigh_layer1_list[i][0][0][j]\n",
        "  confidence0 = times0/5*distance0\n",
        "  confidence1 = times1/5*distance1\n",
        "  list_confidence0.append(confidence0)\n",
        "  list_confidence1.append(confidence1)\n",
        "  #delete to 0\n",
        "  confidence0 = 0\n",
        "  confidence1 = 0\n",
        "  distance0 = 0\n",
        "  distance1 = 0\n",
        "  times0 = 0\n",
        "  times1 = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDdZ4fzAFCTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "con_label = []\n",
        "for i in range(len(list_confidence0)):\n",
        "  if list_confidence0[i] > list_confidence1[i]:\n",
        "    con_label.append([list_confidence0[i],\"0\"])\n",
        "  if list_confidence0[i] < list_confidence1[i]:\n",
        "    con_label.append([list_confidence1[i],\"1\"])\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cozb5c7FH7Ie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b49a7324-8d6c-48ed-be4c-e2db32b1d144"
      },
      "source": [
        "len(con_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSKoRuf0hnJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "con_label"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}